# UkulelePro â€” Project Brief & Development Strategy
**Version:** 1.0  
**Language:** Swift (iOS First)  
**Target Platform:** iPhone â†’ iPad â†’ macOS â†’ Android (future)  
**Prepared For:** Antigravity IDE â†’ Xcode IDE Handoff  

---

## 1. Project Vision

**UkulelePro** is a native mobile application that serves as an all-in-one musician's companion for ukulele players. It combines three core capabilities into a single, polished experience:

1. **Real-time chromatic tuner** for Baritone and Tenor ukulele
2. **Live chord detection** â€” listening to audio and identifying chords as they are played
3. **Song performance companion** â€” displaying lyrics and graphical chord diagrams synchronized to detected or pre-loaded chord changes in real time

The visual identity of the app centers around **graphical chord fretboard diagrams** (not tab notation) rendered natively in SwiftUI â€” similar to what you would find in a printed fake book or chord dictionary.

---

## 2. Supported Instruments & Tunings

| Instrument | String Tuning (Low â†’ High) | Notes |
|---|---|---|
| Tenor Ukulele | G â€“ C â€“ E â€“ A | Standard; Low-G option supported |
| Tenor Ukulele (Low-G) | G â€“ C â€“ E â€“ A | G string one octave lower |
| Baritone Ukulele | D â€“ G â€“ B â€“ E | Same as top 4 strings of guitar |

The active tuning profile is user-selectable and affects both the tuner reference pitches and the chord voicing database served to the diagram renderer.

---

## 3. Feature Modules

### 3.1 â€” Chromatic Tuner
- Real-time pitch detection via microphone
- Displays nearest note name, cents deviation, and octave
- Visual needle / arc indicator (flat â—€ | in tune âœ“ | sharp â–¶)
- Supports both tuning profiles simultaneously (switch via tab or picker)
- Target accuracy: Â±1 cent

### 3.2 â€” Chord Detection Engine
- Listens to live audio from microphone
- Analyzes chroma vectors in real time (FFT â†’ 12-bin pitch class energy)
- Pattern-matches against chord template library
- Displays detected chord name + graphical diagram
- Confidence threshold filter (suppresses noise / ambiguous detections)
- Chord detection history shown as a scrolling timeline

### 3.3 â€” Song / Lyric Companion
- Two modes: **Live Listen Mode** and **Pre-loaded Song Mode**
- Displays lyrics line by line with chord diagram annotations above the relevant syllable/word
- Chord diagrams animate in real time as changes are detected or triggered
- Song data stored as structured JSON (lyrics + timestamps + chord names)
- Simple song library with search, favorites, and import

### 3.4 â€” Chord Dictionary
- Full reference library of all chord voicings for both tunings
- Organized by root (C, C#, D â€¦ B) and chord type
- Graphical fretboard diagram for every entry
- Chord types covered (per root, Ã—12 = total library):

| Family | Types |
|---|---|
| Triads | major, minor, augmented, diminished |
| Sevenths | dom7, maj7, min7, dim7, half-dim7 (m7b5), minMaj7 |
| Extended | 9, maj9, min9, add9, 11, maj11, 13, maj13 |
| Suspended | sus2, sus4, 7sus4 |
| Altered | 7b5, 7#5, 7b9, 7#9, 7#11 |
| Added | add9, add11, 6, m6, 6/9 |

**Estimated total:** ~360 chord shapes per tuning, ~720 total entries across both tunings.

---

## 4. Technical Architecture

### 4.1 High-Level Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               SwiftUI Layer                 â”‚
â”‚  TunerView | ChordView | LyricView | DictViewâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           App Logic / ViewModels            â”‚
â”‚  TunerViewModel | ChordDetectionViewModel   â”‚
â”‚  SongSessionViewModel | LibraryViewModel    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AudioKit 5 â”‚         â”‚   ChordEngine.swift â”‚
â”‚  (AudioEngineâ”‚        â”‚   (Pure Swift DSP)  â”‚
â”‚  PitchTap   â”‚        â”‚   vDSP + Accelerate â”‚
â”‚  FFTTap)    â”‚         â”‚   ChromaVector      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   ChordMatcher      â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”˜
                                 â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  ChordDatabase.jsonâ”‚
                        â”‚  (Tenor + Baritone)â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 Audio Pipeline Detail

```
Microphone Input
     â”‚
     â–¼
AudioEngine (AudioKit) â€” low-latency buffer capture
     â”‚
     â”œâ”€â”€â–¶ PitchTap â”€â”€â–¶ YIN Algorithm â”€â”€â–¶ TunerViewModel (Hz â†’ Note â†’ Cents)
     â”‚
     â””â”€â”€â–¶ FFTTap â”€â”€â–¶ Magnitude Spectrum
                         â”‚
                         â–¼
                    Hann Windowing (vDSP)
                         â”‚
                         â–¼
                    Frequency Binning â†’ 12 Pitch Classes
                         â”‚
                         â–¼
                    Chroma Vector [C,C#,D,D#,E,F,F#,G,G#,A,A#,B]
                         â”‚
                         â–¼
                    Cosine Similarity vs Chord Templates
                         â”‚
                         â–¼
                    Top-N Match â†’ ChordDetectionViewModel
```

### 4.3 Module Breakdown

#### ChordEngine.swift
The core DSP module, written in pure Swift using Apple's `Accelerate` / `vDSP` framework. No external dependency for this step. Responsibilities:

- Buffer windowing (Hann)
- FFT via `vDSP.FFT`
- Frequency-to-pitch-class binning (equal temperament, A4=440Hz)
- Chroma vector normalization
- Template matching (cosine similarity)
- Output: `ChordDetectionResult(chordName: String, confidence: Float, chromaVector: [Float])`

#### ChordDiagramView.swift (SwiftUI Canvas)
Renders a guitar/uke fretboard diagram from a `ChordShape` model:

```swift
struct ChordShape: Codable {
    let name: String          // "Cmaj7"
    let tuning: Tuning        // .tenor | .baritone
    let frets: [Int?]         // nil = muted X, 0 = open O, n = fret number
    let fingers: [Int?]       // finger numbers 1-4
    let barre: BarreShape?    // optional { fret: Int, fromString: Int, toString: Int }
    let startFret: Int        // for high-up shapes (shows fret number label)
}
```

Diagram elements drawn via SwiftUI `Canvas`:
- Nut (thick top line if `startFret == 1`)
- Fret lines (4 frets displayed by default)
- String lines (4 strings)
- Filled circles for finger positions
- Barre arc / rectangle for barre chords
- "O" above open strings, "X" above muted strings
- Fret number label if `startFret > 1`

#### ChordDatabase.json
Pre-built JSON file bundled with the app. Schema:

```json
{
  "tenor": {
    "C": {
      "major": { "frets": [0,0,0,3], "fingers": [null,null,null,3], "barre": null, "startFret": 1 },
      "minor": { "frets": [0,3,3,3], "fingers": [null,1,2,3], "barre": null, "startFret": 1 },
      ...
    },
    ...
  },
  "baritone": { ... }
}
```

Generation strategy: Write a Swift command-line tool (`ChordGen`) that applies music theory rules to programmatically generate voicings for each tuning, then hand-verify edge cases.

#### SongLibrary
Song data format:

```json
{
  "title": "Somewhere Over the Rainbow",
  "tuning": "tenor",
  "bpm": 76,
  "sections": [
    {
      "lyrics": "Somewhere over the rainbow",
      "chords": [
        { "chord": "C", "wordIndex": 0, "beatOffset": 0.0 },
        { "chord": "Em", "wordIndex": 2, "beatOffset": 2.0 }
      ]
    }
  ]
}
```

---

## 5. Toolchain & Dependencies

### 5.1 Primary Dependencies

| Dependency | Version | Source | Purpose |
|---|---|---|---|
| **AudioKit** | 5.x | Swift Package Manager | Audio engine, PitchTap, FFTTap |
| **Accelerate (vDSP)** | System | Apple SDK (built-in) | FFT, DSP math |
| **AVFoundation** | System | Apple SDK (built-in) | Microphone access, session management |
| **Speech** | System | Apple SDK (built-in) | Optional live lyric transcription |
| **SwiftUI** | System | Apple SDK (built-in) | All UI including chord diagrams |
| **Combine** | System | Apple SDK (built-in) | Reactive data binding |

### 5.2 Optional / Future Dependencies

| Dependency | Purpose | When |
|---|---|---|
| **Essentia** (C++ via ObjC bridge) | Higher-accuracy chord detection | Phase 2 if needed |
| **MusicKit** | Apple Music integration, lyrics API | Phase 2 |
| **CloudKit** | iCloud song library sync | Phase 2 |
| **Kotlin Multiplatform Mobile (KMM)** | Cross-platform logic layer | Android port phase |

### 5.3 Xcode Project Settings

```
iOS Deployment Target:    iOS 16.0+
Swift Version:            5.9+
Supported Devices:        iPhone (primary), iPad (adaptive layout)
Capabilities Required:
  - Microphone Usage (NSMicrophoneUsageDescription)
  - Speech Recognition (NSSpeechRecognitionUsageDescription)
Bundle Identifier:        com.[studio].ukulepro
SwiftUI Previews:         Enabled
Package Manager:          Swift Package Manager (no CocoaPods)
```

### 5.4 Info.plist Keys Required

```xml
<key>NSMicrophoneUsageDescription</key>
<string>UkulelePro needs microphone access to tune your ukulele and detect chords.</string>

<key>NSSpeechRecognitionUsageDescription</key>
<string>UkulelePro uses speech recognition to transcribe lyrics in real time.</string>
```

---

## 6. Project Structure (Xcode)

```
UkulelePro/
â”œâ”€â”€ App/
â”‚   â”œâ”€â”€ UkuleleProApp.swift
â”‚   â””â”€â”€ ContentView.swift
â”œâ”€â”€ Features/
â”‚   â”œâ”€â”€ Tuner/
â”‚   â”‚   â”œâ”€â”€ TunerView.swift
â”‚   â”‚   â””â”€â”€ TunerViewModel.swift
â”‚   â”œâ”€â”€ ChordDetection/
â”‚   â”‚   â”œâ”€â”€ ChordDetectionView.swift
â”‚   â”‚   â”œâ”€â”€ ChordDetectionViewModel.swift
â”‚   â”‚   â””â”€â”€ ChordEngine.swift          â† Core DSP logic
â”‚   â”œâ”€â”€ SongCompanion/
â”‚   â”‚   â”œâ”€â”€ SongCompanionView.swift
â”‚   â”‚   â”œâ”€â”€ SongCompanionViewModel.swift
â”‚   â”‚   â””â”€â”€ LyricLineView.swift
â”‚   â””â”€â”€ ChordDictionary/
â”‚       â”œâ”€â”€ ChordDictionaryView.swift
â”‚       â””â”€â”€ ChordSearchViewModel.swift
â”œâ”€â”€ Shared/
â”‚   â”œâ”€â”€ Views/
â”‚   â”‚   â””â”€â”€ ChordDiagramView.swift     â† Reusable diagram renderer
â”‚   â”œâ”€â”€ Models/
â”‚   â”‚   â”œâ”€â”€ ChordShape.swift
â”‚   â”‚   â”œâ”€â”€ Song.swift
â”‚   â”‚   â”œâ”€â”€ Tuning.swift
â”‚   â”‚   â””â”€â”€ ChordDetectionResult.swift
â”‚   â”œâ”€â”€ Audio/
â”‚   â”‚   â””â”€â”€ AudioSessionManager.swift
â”‚   â””â”€â”€ Data/
â”‚       â”œâ”€â”€ ChordDatabase.swift        â† Loads and queries JSON
â”‚       â””â”€â”€ SongLibrary.swift
â”œâ”€â”€ Resources/
â”‚   â”œâ”€â”€ ChordDatabase.json
â”‚   â””â”€â”€ SampleSongs/
â”‚       â””â”€â”€ *.json
â””â”€â”€ Tools/
    â””â”€â”€ ChordGen/                      â† CLI tool for chord generation
        â””â”€â”€ main.swift
```

---

## 7. Development Phases

### Phase 1 â€” Foundation (Weeks 1â€“3)
- [ ] Xcode project scaffold with SwiftUI tabs
- [ ] AudioKit integration and microphone pipeline
- [ ] Tuner: pitch detection â†’ display (both tunings)
- [ ] `ChordDiagramView` renderer in SwiftUI Canvas (hardcoded test chord)
- [ ] Basic `ChordDatabase.json` (C, G, Am, F for both tunings as proof of concept)

### Phase 2 â€” Chord Engine (Weeks 4â€“6)
- [ ] `ChordEngine.swift`: FFT â†’ chroma vector â†’ template matching
- [ ] Real-time chord detection display with confidence score
- [ ] Full `ChordDatabase.json` generation via `ChordGen` tool (all ~720 shapes)
- [ ] Chord Dictionary view: browse, search, filter by tuning

### Phase 3 â€” Song Companion (Weeks 7â€“9)
- [ ] Song JSON schema + parser
- [ ] Pre-loaded song mode: lyrics + chord diagram timeline
- [ ] Live mode: detected chords update diagram in real time alongside lyrics
- [ ] 5â€“10 built-in sample songs

### Phase 4 â€” Polish & Extras (Weeks 10â€“12)
- [ ] App icon, launch screen, design system (colors, typography)
- [ ] Onboarding flow (tuning selection, permissions)
- [ ] Song import (user-created JSON or simple text import)
- [ ] Optional: Speech recognition lyric capture
- [ ] TestFlight beta

### Phase 5 â€” Cross-Platform Prep (Future)
- [ ] Extract `ChordEngine`, `ChordDatabase`, and `Song` models into a Kotlin Multiplatform module
- [ ] Android app shell consuming KMM business logic
- [ ] SwiftUI â†’ Jetpack Compose UI port

---

## 8. Known Technical Challenges & Mitigation

| Challenge | Risk | Mitigation |
|---|---|---|
| Polyphonic chord detection accuracy | Medium-High | Use chroma-based detection with confidence thresholding; suppress low-confidence outputs; optionally integrate Essentia later |
| Distinguishing enharmonic chords (e.g. Dm7 vs F6) | High | Accept ambiguity with a "best match" UX â€” show top 2 candidates |
| Background noise affecting detection | Medium | Apply noise gating (amplitude threshold) before analysis; only process frames above a dB floor |
| Real-time lyric transcription accuracy | High | Default to pre-loaded song mode first; live transcription is a Phase 4+ enhancement |
| Generating all 720 chord shapes correctly | Medium | Use the `ChordGen` CLI tool + music theory validation; allow community correction via feedback |
| AudioKit API changes | Low | Pin AudioKit version in SPM; monitor release notes |

---

## 9. Architecture Principles & Coding Standards

- **MVVM** throughout â€” Views own no business logic
- **Combine** for all reactive bindings between Audio layer and ViewModels
- **@MainActor** on all ViewModels (audio callbacks post to main queue)
- **Swift Concurrency (async/await)** for database loading and file I/O
- No third-party UI libraries â€” SwiftUI only, keep the app lean
- `ChordEngine` must be fully **unit testable** with synthetic chroma vectors
- All chord data **externalized in JSON** â€” never hardcoded in Swift

---

## 10. Cross-Platform Portability Notes

When writing Swift code, apply these conventions to make future porting to Android/Kotlin easier:

- Keep DSP math in pure functions with no SwiftUI dependency
- `ChordEngine` should be a stateless transform: `[Float] â†’ ChordDetectionResult`
- All model types (`ChordShape`, `Song`, `ChordDetectionResult`) should be simple value types â€” no platform APIs in models
- Consider tagging cross-platform-safe files with `// PORTABLE` comment
- When Android port begins, use **Kotlin Multiplatform Mobile (KMM)** to share models + ChordEngine logic; rewrite UI natively in Jetpack Compose

---

## 11. Summary â€” What We Are Building

> A native iPhone app that can **tune** a baritone or tenor ukulele, **listen** to someone playing and identify the chords in real time, and **display** the lyrics of a song alongside beautiful graphical chord diagrams that update live as the music plays â€” serving as an intelligent, visual cheat-sheet for ukulele players of all skill levels.

**The three pillars:**
- ðŸŽµ **Tuner** â€” precise, clean, fast
- ðŸŽ¸ **Chord Detector** â€” DSP-powered, real time
- ðŸ“– **Song Companion** â€” lyrics + diagrams, synchronized

**The secret weapon:**
A hand-crafted, comprehensive chord voicing database for both tunings covering every common chord type â€” rendered as beautiful, scalable, graphical fretboard diagrams using nothing but SwiftUI's Canvas API.

---

*Document prepared for handoff to Antigravity IDE. All module names, file paths, and JSON schemas are intended as concrete implementation targets, not suggestions.*
